# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193PzgDPdoJXvebq5XvEfm8XxlxIzYUHB
"""

import json
import asyncio
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv

load_dotenv() 

os.environ["OPENAI_API_KEY"] = os.getenv("OPEN_AI_KEY")

async def main():
    data={ "question": "Are there any minimal GRE or TOEFL scores needed to get admitted into the MS programs? ",
            "context": '''Our department imposes no minimal score requirements, but please do note that the Rutgers Graduate School
            publishes minimal requirements for English proficiency. See http://gradstudy.rutgers.edu/information/international-students. During the admission process, we look at all the submitted material.
            Clearly, the better the test scores, the better the chances to get admitted.'''
        },{
            "question": "Can Rutgers or the CS Department provide funding?",
            "context": '''University or Department funding can not be offered to MS students at Rutgers. We expect MS students
            to make their own financial arrangements. However, individual research groups are sometimes able to provide funding
            on a case-by-case basis to admitted MS students through their research grants. Your academic advisor can give
            you advice about contacting individual
            faculty members about opportunities for support in their research group.'''
        }

    async def ask_gpt(faq_text, question):
        sys_message = "You are a helpful assistant specialized in generating questions for which the answer can be found in the document text."
        prompt = f'''Generate a list of 10 questions, similar to the question {question}, that can be answered based on the following text:
        \n\n{faq_text}.
        Do not produce anything else in the output, just a list'''

        # Appends the prompt to chatlog. 
        chatlog = [{
            'role': 'system',
            'content': sys_message,
        }]
        chatlog.append({'role': 'user', 'content': prompt})

        # Creates openAI client.
        client = AsyncOpenAI()
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=chatlog,
            max_tokens=1000,
            n=1,
            stop=None,
            temperature=1.2,
        )

        # Prints reponse.
        print("\n")
        print(response)

        # Extracts questions from response.
        questions = response.choices[0].message.content.strip().split("\n")
        return questions

    # Opens output file for writing
    output_file = open("./generated_rutgers_FAQ.json", 'w')

    # Appends questions to segmented data.
    segmented_data = []
    for item in data:
        question = item['question']
        context = item['context']
        generated_questions = await ask_gpt(context, question)
        segmented_data.append({"generated_questions": generated_questions, "context": context})

    # Writes to output file in json format.
    json.dump(segmented_data, output_file, indent=4)
    output_file.close()

    # Reads file just written from. Appends questiosn to quest_group, context to context.
    question_groups = []
    contexts = []
    file = open("./generated_rutgers_FAQ.json", 'r')
    segmented_data = json.load(file)
    for item in segmented_data:
        question_groups.append(item["generated_questions"])
        contexts.append(item["context"])

    #Use BM25, prepares BM25 for use.
    from rank_bm25 import BM25Okapi
    from nltk.tokenize import word_tokenize
    import nltk
    nltk.download('punkt_tab')

    # #Use LanceDB
    # !pip install lancedb
    from lancedb.pydantic import LanceModel, Vector

    nltk.download('punkt')

    # Tokenize text for BM25
    def tokenize(text):
        return word_tokenize(text.lower())

    # tokenized questions put into BM25.
    tokenized_questions = [tokenize(' '.join(questions)) for questions in question_groups]
    print("\n")
    print(tokenized_questions)
    bm25 = BM25Okapi(tokenized_questions)
    retriever = {
        "api": bm25,
        "question_groups": question_groups,
        "question_embeddings": tokenized_questions,
        "contexts": contexts
    }

    best_context = ""
    contexts = retriever["contexts"]
    print("\n")
    print(contexts)
    question_groups = retriever["question_groups"]
    print("\n")
    print(question_groups)
    tokenized_query = tokenize(question)
    print("\n")
    print(tokenized_query)
    scores = bm25.get_scores(tokenized_query)
    print("\n")
    print(scores)
    best_match_index = scores.argmax()
    best_context = contexts[best_match_index]
asyncio.run(main())
