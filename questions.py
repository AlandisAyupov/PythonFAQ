# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193PzgDPdoJXvebq5XvEfm8XxlxIzYUHB
"""

data=
       { "question": "Are there any minimal GRE or TOEFL scores needed to get admitted into the MS programs? ",
        "context": '''Our department imposes no minimal score requirements, but please do note that the Rutgers Graduate School
        publishes minimal requirements for English proficiency. See http://gradstudy.rutgers.edu/information/international-students. During the admission process, we look at all the submitted material.
        Clearly, the better the test scores, the better the chances to get admitted.'''
    },
    {
        "question": "Can Rutgers or the CS Department provide funding?",
        "context": '''University or Department funding can not be offered to MS students at Rutgers. We expect MS students
        to make their own financial arrangements. However, individual research groups are sometimes able to provide funding
        on a case-by-case basis to admitted MS students through their research grants. Your academic advisor can give
        you advice about contacting individual
        faculty members about opportunities for support in their research group.'''
    }

def ask_gpt(faq_text, question):
    sys_message = "You are a helpful assistant specialized in generating questions for which the answer can be found in the document text."
    prompt = f'''Generate a list of 10 questions, similar to the question {question}, that can be answered based on the following text:
    \n\n{faq_text}.
     Do not produce anything else in the output, just a list'''

    chatlog = [{
        'role': 'system',
        'content': sys_message,
    }]
    chatlog.append({'role': 'user', 'content': prompt})

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=chatlog,
        max_tokens=1000,
        n=1,
        stop=None,
        temperature=1.2,
    )

    questions = response.choices[0].message.content.strip().split("\n")
    return questions

output_file = open("/generated_rutgers_FAQ.json", 'w')


segmented_data = []
for item in data:
    question = item['question']
    context = item['context']
    generated_questions = ask_gpt(context, question)
    segmented_data.append({"generated_questions": generated_questions, "context": context})

json.dump(segmented_data, output_file, indent=4)
output_file.close()

question_groups = []
contexts = []
file = open("/generated_rutgers_FAQ.json", 'r')
segmented_data = json.load(file)
for item in segmented_data:
    question_groups.append(item["generated_questions"])
    contexts.append(item["context"])

#Use BM25
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize
import nltk

# #Use LanceDB
# !pip install lancedb
from lancedb.pydantic import LanceModel, Vector

nltk.download('punkt')

# Tokenize text for BM25
def tokenize(text):
    return word_tokenize(text.lower())

tokenized_questions = [tokenize(' '.join(questions)) for questions in question_groups]
bm25 = BM25Okapi(tokenized_questions)
retriever = {
    "api": bm25,
    "question_groups": question_groups,
    "question_embeddings": tokenized_questions,
    "contexts": contexts
}

best_context = ""
contexts = retriever["contexts"]
question_groups = retriever["question_groups"]
tokenized_query = tokenize(question)
scores = bm25.get_scores(tokenized_query)
best_match_index = scores.argmax()
best_context = contexts[best_match_index]

